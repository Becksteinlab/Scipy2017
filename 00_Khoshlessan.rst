.. -*- mode: rst; fill-column: 9999; coding: utf-8 -*-

:author: Mahzad Khoshlessan
:email: mkhoshle@asu.edu
:institution: Arizona State University

:author: Oliver Beckstein
:email: obeckste@asu.edu 
:institution: Arizona State University 
:corresponding:

-------------------------------------------------------------------------
Parallel Analysis in MDAnalysis using the Dask Parallel Computing Library
-------------------------------------------------------------------------

Brief abstract
--------------

The analysis of biomolecular computer simulations has become a challenge because the amount of output data is now routinely in the terabyte range.
We evaluate if this challenge can be met by a parallel map-reduce approach with the Dask_ parallel computing library for task-graph based computing coupled with our MDAnalysis_ Python library for the analysis of molecular dynamics (MD) simulations [Gowers2016]_.
We performed a representative performance evaluation, taking into account the highly heterogenous computing environment that researchers typically work in together with the diversity of existing file formats for MD trajectory data.
We found that the the underlying storage system (solid state drives, parallel file systems, or simple spinning platter disks) can be a deciding performance factor that leads to data ingestion becoming the primary bottle neck in the analysis work flow.
However, the choice of the data file format can mitigate the effect of the storage system; in particular, the commonly used "Gromacs XTC" trajectory format, which is highly compressed, can exhibit strong scaling close to ideal due to trading a decrease in global storage access load against and increase in local per-core cpu-intensive decompression.
Scaling was tested on single nodes and multiple nodes on national and local supercomputing resources as well as typical workstations.
In summary, we show that, due to the focus on high interoperability in the scientific Python eco system, it is straightforward to implement map-reduce with Dask in MDAnalysis and provide an in-depth analysis of the considerations to obtain good parallel performance on HPC resources.


Long description
----------------

MDAnalysis_ is a Python library that provides users with access to raw simulation data that allows structural and temporal analysis of molecular dynamics (MD) trajectories generated by all major MD simulation packages. 
The size of these trajectories is growing as the simulation times is being extended from micro-seconds to mili-seconds. Thus the amount of data to be analyzed is growing rapidly and analysis is increasingly becoming a bottleneck. 
Therefore, there is a need for high performance computing (HPC) approaches to increase the throughput. MDAnalysis does not yet provide a standard interface for parallel analysis; instead, various existing parallel libraries are currently used to parallelize MDAnalysis-based code.
Present study aims to identify possible approaches for bringing HPC into MDAnalysis. In this paper, we evaluate performance for parallel map-reduce type analysis along with the Dask_ parallel computing library for task-graph based distributed computing. 
A range of commonly used MD file formats (CHARMM/NAMD DCD, Gromacs XTC, Amber NetCDF) and different trajectory sizes are benchmarked on different high performance computing (HPC) resources including national supercomputers (XSEDE TACC Stampede and SDSC Comet), university supercomputers (ASU Research computing center (Saguaro)), and local resources (Gigabit networked multi-core workstations). 
All resources architectures are parallel and heterogeneous with different CPUs, file systems, high speed networks, for high-performance distributed computing. 
This heterogeneous environment creates a challenging problem for developing high performance programs without the effort required to use low-level, architecture specific parallel programming models for our domain-specific problem. 
Different storages like solid state drives (SSDs), hard disk drives (HDDs) and Lustre file system are also tested to examine effect of storage location on the I/O performance. 
The benchmarks are performed both on a single node and across multiple nodes using multiprocessing and distributed schedulers in Dask library.
Results are compared across all file format, trajectory sizes and all machines. 
Overall, our results show strong dependency to hardware and file formats. On a single node, we found slightly better scaling than multiple nodes. 
Our results show that there are other challenges aside from I/O bottleneck for achieving good speed-up. According to our results, although Map-Reduce job is pleasantly parallel and all processors have the same amount of work to do, some of the processes are much slower than the others in some tests. 
We hypothesize that weak performance gains may be due to external resource like contention on the network that may slow down individual tasks and lead to overall waits and poor load balancing. 
Also, we hypothesize that we may be able to make our code more robust to uncertainty in computations by submitting many more tasks than the number of cores. 
This will allow Dask to load balance appropriately, covering the wait time for faster processes when some tasks take longer than expected. 
Present study provides guidelines for how the choice of trajectory format along with the hardware can lead to good performance. 
At the end, we also suggest the promising approaches, including using file striping, loading data in chunks into memory, using heavier analyses to balance data requests across all CPUs, and parallel I/O,  that can be implemented to get the best possible performance out of MDAnalysis library.


Keywords
--------
MDAnalysis, High Performance Computing, Dask, Map-Reduce


The draft of the present study including all of our data is available on figshare at DOI: `10.6084/m9.figshare.4695742`_; 

Data Files
----------

The topology file (PSF format) and the trajectory (DCD) used for the benchmark
can be downloaded from dropbox

- `adk4AKE.psf`_
- `1ake_007-nowater-core-dt240ps.dcd`_

Files in XTC and NetCDF formats are generated from the DCD.

Tested libraries
----------------

- MDAnalysis_ 0.15.0
- Dask_ 0.12.0 (also 0.13.0)
- Distributed_ 1.14.3 (also 1.15.1)
- NumPy_ 1.11.2 (also 1.12.0)


References
----------

.. [Gowers2016] R. J. Gowers, M. Linke, J. Barnoud, T. J. E. Reddy, M. N. Melo, S. L. Seyler, D. L. Dotson, J. Doman ́ski, S. Buchoux, I. M. Kenney, and O. Beckstein. MDAnalysis: A Python package for the rapid analysis of molecular dynamics simulations. In S. Benthall and S. Rostrup, editors, Proceedings of the 15th Python in Science Conference, pages 102 – 109, Austin, TX, 2016. SciPy. URL http://mdanalysis.org.

.. [Khoshlessan2017] Khoshlessan, Mahzad; Beckstein, Oliver (2017): Parallel analysis in the MDAnalysis Library: Benchmark of Trajectory File Formats. figshare. doi:`10.6084/m9.figshare.4695742`_


.. _MDAnalysis: http://mdanalysis.org
.. _Dask: http://dask.pydata.org
.. _Distributed: https://distributed.readthedocs.io/
.. _NumPy: http://numpy.scipy.org/
.. _`10.6084/m9.figshare.4695742`: https://doi.org/10.6084/m9.figshare.4695742
.. _`adk4AKE.psf`: https://www.dropbox.com/sh/ln0klc9j7mhvxkg/AAAL5eP1vrn0tK-67qVDnKeua/Trajectories/equilibrium/adk4AKE.psf
.. _`1ake_007-nowater-core-dt240ps.dcd`: https://www.dropbox.com/sh/ln0klc9j7mhvxkg/AABSaNJ0fRFgY1UfxIH_jWtka/Trajectories/equilibrium/1ake_007-nowater-core-dt240ps.dcd
